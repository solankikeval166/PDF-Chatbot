{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f90d69d",
   "metadata": {},
   "source": [
    "# Small RAG from Local data without LLMs based on jaccard_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fecb3a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_of_documents = [\n",
    "    \"Take a leisurely walk in the park and enjoy the fresh air.\",\n",
    "    \"Visit a local museum and discover something new.\",\n",
    "    \"Attend a live music concert and feel the rhythm.\",\n",
    "    \"Go for a hike and admire the natural scenery.\",\n",
    "    \"Have a picnic with friends and share some laughs.\",\n",
    "    \"Explore a new cuisine by dining at an ethnic restaurant.\",\n",
    "    \"Take a yoga class and stretch your body and mind.\",\n",
    "    \"Join a local sports league and enjoy some friendly competition.\",\n",
    "    \"Attend a workshop or lecture on a topic you're interested in.\",\n",
    "    \"Visit an amusement park and ride the roller coasters.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5342caff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(query, document):\n",
    "    query = query.lower().split(\" \")\n",
    "    document = document.lower().split(\" \")\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60ab66d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_response(query, corpus):\n",
    "    similarities = []\n",
    "    for doc in corpus:\n",
    "        similarity = jaccard_similarity(query, doc)\n",
    "        similarities.append(similarity)\n",
    "    return corpus_of_documents[similarities.index(max(similarities))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b59cc961",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"What is a leisure activity that you like?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72aa4b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"I like to hike\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3ab8c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go for a hike and admire the natural scenery.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_response(user_input, corpus_of_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a9b663",
   "metadata": {},
   "source": [
    "### Problem with jaccard_similarity the meaning was different but the response was same because of common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74546b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"I don't like to hike\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09281504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go for a hike and admire the natural scenery.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_response(user_input, corpus_of_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1b434d",
   "metadata": {},
   "source": [
    "### Now we are using llama3 for finiding best answer for  our query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa557868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "266eafbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"I like to hike\"\n",
    "relevant_document = return_response(user_input, corpus_of_documents)\n",
    "full_response = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2956d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a bot that makes recommendations for activities. You answer in very short sentences and do not include extra information.\n",
    "This is the recommended activity: {relevant_document}\n",
    "The user input is: {user_input}\n",
    "Compile a recommendation to the user based on the recommended activity and the user input.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6f36dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://localhost:11434/api/generate'\n",
    "data = {\n",
    "    \"model\": \"llama3\",\n",
    "    \"prompt\": prompt.format(user_input=user_input, relevant_document=relevant_document)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "149b35a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try a challenging trail!\n"
     ]
    }
   ],
   "source": [
    "headers = {'Content-Type': 'application/json'}\n",
    "response = requests.post(url, data=json.dumps(data), headers=headers, stream=True)\n",
    "try:\n",
    "    count = 0\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            decoded_line = json.loads(line.decode('utf-8'))\n",
    "            \n",
    "            full_response.append(decoded_line['response'])\n",
    "finally:\n",
    "    response.close()\n",
    "print(''.join(full_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17de7f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go for a hike and admire the natural scenery.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c238711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"I don't like to hike\"\n",
    "relevant_document = return_response(user_input, corpus_of_documents)\n",
    "full_response = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "625f72b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try birdwatching instead.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "You are a bot that makes recommendations for activities. You answer in very short sentences and do not include extra information.\n",
    "This is the recommended activity: {relevant_document}\n",
    "The user input is: {user_input}\n",
    "Compile a recommendation to the user based on the recommended activity and the user input.\n",
    "\"\"\"\n",
    "\n",
    "url = 'http://localhost:11434/api/generate'\n",
    "\n",
    "data = {\n",
    "    \"model\": \"llama3\",\n",
    "    \"prompt\": prompt.format(user_input=user_input, relevant_document=relevant_document)\n",
    "}\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "response = requests.post(url, data=json.dumps(data), headers=headers, stream=True)\n",
    "try:\n",
    "    for line in response.iter_lines():\n",
    "        # filter out keep-alive new lines\n",
    "        if line:\n",
    "            decoded_line = json.loads(line.decode('utf-8'))\n",
    "            # print(decoded_line['response'])  # uncomment to results, token by token\n",
    "            full_response.append(decoded_line['response'])\n",
    "finally:\n",
    "    response.close()\n",
    "print(''.join(full_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f053c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_response.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd82b8d",
   "metadata": {},
   "source": [
    "### Using own PDF as  a documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89c9affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as timer\n",
    "\n",
    "from langchain.llms import Ollama\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b2d7c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./AttentionIsAllYouNeed.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "93dae2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3d5f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model='llama3',\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "664d216c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Evoortsolutions\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the pre-trained model you want to use\n",
    "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "\n",
    "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "model_kwargs = {'device':'cpu'}\n",
    "\n",
    "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5ee1cf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(documents=all_splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "643eca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. Please follow the following rules:\n",
    "1. If you don't know the answer, don't try to make up an answer. Just say \"I can't find the final answer but you may want to check the following links\".\n",
    "2. If you find the answer, write the answer in a concise way with five sentences maximum.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    " template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Define RetrievalQA chain\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm, \n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    verbose=True,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\":PROMPT}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f49c3369",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Explain about Encoder and Decoder Stacks in Transformers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "628e8c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Explain about Encoder and Decoder Stacks in Transformers\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "According to the given context, the Transformer's architecture follows a stacked self-attention mechanism with point-wise, fully connected layers for both the encoder and decoder.\n",
      "\n",
      "The Encoder Stack is composed of 6 identical layers, each containing two sub-layers: multi-head self-attention and a simple, position-wise fully connected feed-forward network. Each layer employs residual connections around the two sub-layers, followed by layer normalization.\n",
      "\n",
      "Similarly, the Decoder Stack also has 6 identical layers, with an additional third sub-layer performing multi-head attention over the output of the encoder stack. The decoder layers also use residual connections and layer normalization.\n",
      "\n",
      "In summary, both the Encoder and Decoder Stacks in Transformers consist of multiple identical layers, each containing self-attention mechanisms and feed-forward networks, with residual connections and layer normalization to facilitate these processes.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "Elapsed time: 387.10440945625305\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query: {query}\")\n",
    "# docs = vectorstore.similarity_search(query)\n",
    "# print(f\"Docs (similarity search results): {docs}\")\n",
    "\n",
    "# Run the chain\n",
    "start_t = timer.time()\n",
    "response = chain({\"query\": query})\n",
    "elapsed_t = timer.time() - start_t\n",
    "print(f\"\\n\\nElapsed time: {elapsed_t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3694d294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain about Encoder and Decoder Stacks in Transformers\n"
     ]
    }
   ],
   "source": [
    "print(response['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b013bc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(response['source_documents']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "23e46d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 documents retrieved which are relevant to the query.\n",
      "****************************************************************************************************\n",
      "Relevant Document #1:\n",
      "Source file: ./AttentionIsAllYouNeed.pdf, Page: 2\n",
      "Content: Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\n",
      "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "wise fully connected feed-forward network. We employ a residual connection [ 11] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There are 4 documents retrieved which are relevant to the query.\n",
      "Relevant Document #2:\n",
      "Source file: ./AttentionIsAllYouNeed.pdf, Page: 2\n",
      "Content: LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\n",
      "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
      "layers, produce outputs of dimension dmodel = 512 .\n",
      "Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\n",
      "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
      "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
      "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There are 4 documents retrieved which are relevant to the query.\n",
      "Relevant Document #3:\n",
      "Source file: ./AttentionIsAllYouNeed.pdf, Page: 4\n",
      "Content: dk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\n",
      "is similar to that of single-head attention with full dimensionality.\n",
      "3.2.3 Applications of Attention in our Model\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows every\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[38, 2, 9].\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There are 4 documents retrieved which are relevant to the query.\n",
      "Relevant Document #4:\n",
      "Source file: ./AttentionIsAllYouNeed.pdf, Page: 7\n",
      "Content: Transformer (big) 28.4 41.8 2.3·1019\n",
      "Residual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\n",
      "sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\n",
      "positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\n",
      "Pdrop= 0.1.\n",
      "Label Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\n",
      "hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "6 Results\n",
      "6.1 Machine Translation\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There are 4 documents retrieved which are relevant to the query.\n"
     ]
    }
   ],
   "source": [
    "def print_source_docs(response):\n",
    "    relevant_docs = response['source_documents']\n",
    "    print(f'There are {len(relevant_docs)} documents retrieved which are relevant to the query.')\n",
    "    print(\"*\" * 100)\n",
    "    for i, doc in enumerate(relevant_docs):\n",
    "        print(f\"Relevant Document #{i+1}:\\nSource file: {doc.metadata['source']}, Page: {doc.metadata['page']}\\nContent: {doc.page_content}\")\n",
    "        print(\"-\"*100)\n",
    "        print(f'There are {len(relevant_docs)} documents retrieved which are relevant to the query.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b4d6f4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "The answer is that without scaling, the dot products can grow large in magnitude as the value of `dk` increases, pushing the softmax function into regions where it has extremely small gradients. This makes the attention mechanism less effective for larger values of `dk`. To counteract this effect, we scale the dot products by 1/√dk, which helps to maintain a reasonable range of values and ensures that the attention mechanism remains effective even for large values of `dk`.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "Elapsed time: 330.919091463089\n"
     ]
    }
   ],
   "source": [
    "query = \"Why we need scaled-dot product?\"\n",
    "\n",
    "# Run the chain\n",
    "start_t = timer.time()\n",
    "response = chain({\"query\": query})\n",
    "elapsed_t = timer.time() - start_t\n",
    "print(f\"\\n\\nElapsed time: {elapsed_t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1cb8e86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 documents retrieved which are relevant to the query.\n",
      "****************************************************************************************************\n",
      "Relevant Document #1:\n",
      "Source file: ./AttentionIsAllYouNeed.pdf, Page: 3\n",
      "Content: much faster and more space-efficient in practice, since it can be implemented using highly optimized\n",
      "matrix multiplication code.\n",
      "While for small values of dkthe two mechanisms perform similarly, additive attention outperforms\n",
      "dot product attention without scaling for larger values of dk[3]. We suspect that for large values of\n",
      "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
      "extremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\n",
      "3.2.2 Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There are 4 documents retrieved which are relevant to the query.\n",
      "Relevant Document #2:\n",
      "Source file: ./AttentionIsAllYouNeed.pdf, Page: 3\n",
      "Content: Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\n",
      "values.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There are 4 documents retrieved which are relevant to the query.\n",
      "Relevant Document #3:\n",
      "Source file: ./AttentionIsAllYouNeed.pdf, Page: 0\n",
      "Content: has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There are 4 documents retrieved which are relevant to the query.\n",
      "Relevant Document #4:\n",
      "Source file: ./AttentionIsAllYouNeed.pdf, Page: 3\n",
      "Content: we found it beneficial to linearly project the queries, keys and values htimes with different, learned\n",
      "linear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "4To illustrate why the dot products get large, assume that the components of qandkare independent random\n",
      "variables with mean 0and variance 1. Then their dot product, q·k=Pdk\n",
      "i=1qiki, has mean 0and variance dk.\n",
      "4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There are 4 documents retrieved which are relevant to the query.\n"
     ]
    }
   ],
   "source": [
    "relevant_docs = response['source_documents']\n",
    "print(f'There are {len(relevant_docs)} documents retrieved which are relevant to the query.')\n",
    "print(\"*\" * 100)\n",
    "for i, doc in enumerate(relevant_docs):\n",
    "    print(f\"Relevant Document #{i+1}:\\nSource file: {doc.metadata['source']}, Page: {doc.metadata['page']}\\nContent: {doc.page_content}\")\n",
    "    print(\"-\"*100)\n",
    "    print(f'There are {len(relevant_docs)} documents retrieved which are relevant to the query.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718cbebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "response['result']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
